{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, StandardScaler\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Preprocessed_data.csv')\n",
    "\n",
    "#Dividing target variable from the main dataset\n",
    "\n",
    "X = data.iloc[: , 0:-1]\n",
    "Y = data.iloc[: , -1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding categorical data\n",
    "## Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "X_train_cat_encoded = pd.DataFrame(ordinal_encoder.fit_transform(X_train.select_dtypes(exclude='number')))\n",
    "X_train_cat_encoded.columns = X_train.select_dtypes(exclude='number').columns\n",
    "\n",
    "X_test_cat_encoded = pd.DataFrame(ordinal_encoder.transform(X_test.select_dtypes(exclude='number')))\n",
    "X_test_cat_encoded.columns = X_test.select_dtypes(exclude='number').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "Y_train_cat_encoded= pd.DataFrame(label_encoder.fit_transform(Y_train))\n",
    "Y_test_cat_encoded = pd.DataFrame(label_encoder.transform(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_sc=pd.DataFrame(sc.fit_transform(X_train.select_dtypes(exclude='O')))\n",
    "X_test_sc=pd.DataFrame(sc.transform(X_test.select_dtypes(exclude='O')))\n",
    "\n",
    "X_train_sc.columns=X_train.select_dtypes(exclude='O').columns\n",
    "X_test_sc.columns=X_test.select_dtypes(exclude='O').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final=pd.concat([X_train_sc,X_train_cat_encoded],axis=1)\n",
    "X_test_final=pd.concat([X_test_sc,X_test_cat_encoded],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling imbalanced Dataset\n",
    "### Since the dataset is small, will use over-sampling: SMOTE technique to balance the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Using cached imbalanced_learn-0.9.1-py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\soumalla-antara\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.20.3)\n",
      "  Using cached imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\soumalla-antara\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.7.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\soumalla-antara\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\soumalla-antara\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in c:\\users\\soumalla-antara\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.2)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.9.0 imblearn-0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7852, 28), (1476, 28), (7852, 1), (1476, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resample,Y_train_resample=SMOTE(random_state=0,k_neighbors=1).fit_resample(X_train_final,Y_train_cat_encoded)\n",
    "X_test_resample,Y_test_resample=SMOTE(random_state=0,k_neighbors=1).fit_resample(X_test_final,Y_test_cat_encoded)\n",
    "\n",
    "X_train_resample.shape,X_test_resample.shape,Y_train_resample.shape,Y_test_resample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (7852, 28) (7852, 1)\n",
      "Testing dataset shape: (1476, 28) (1476, 1)\n",
      "Training dataset shape: (7852, 28) (7852,)\n",
      "Testing dataset shape: (1476, 28) (1476,)\n"
     ]
    }
   ],
   "source": [
    "print('Training dataset shape:', X_train_resample.shape, Y_train_resample.shape)\n",
    "print('Testing dataset shape:', X_test_resample.shape, Y_test_resample.shape)\n",
    "\n",
    "Y_train_resample_flat = Y_train_resample.to_numpy().ravel()\n",
    "Y_test_resample_flat = Y_test_resample.to_numpy().ravel()\n",
    "\n",
    "print('Training dataset shape:', X_train_resample.shape, Y_train_resample_flat.shape)\n",
    "print('Testing dataset shape:', X_test_resample.shape, Y_test_resample_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward selection approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:   24.4s finished\n",
      "\n",
      "[2022-11-25 01:21:17] Features: 1/10 -- score: 0.7929168406670263[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:   36.8s finished\n",
      "\n",
      "[2022-11-25 01:21:54] Features: 2/10 -- score: 0.9681605695589244[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:   34.5s finished\n",
      "\n",
      "[2022-11-25 01:22:29] Features: 3/10 -- score: 0.9875189238060873[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:   43.3s finished\n",
      "\n",
      "[2022-11-25 01:23:12] Features: 4/10 -- score: 0.9950328201843119[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:   38.4s finished\n",
      "\n",
      "[2022-11-25 01:23:51] Features: 5/10 -- score: 0.9968159353245731[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:   35.0s finished\n",
      "\n",
      "[2022-11-25 01:24:26] Features: 6/10 -- score: 0.9980894963247071[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:   31.7s finished\n",
      "\n",
      "[2022-11-25 01:24:58] Features: 7/10 -- score: 0.9979622699647674[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:   29.2s finished\n",
      "\n",
      "[2022-11-25 01:25:27] Features: 8/10 -- score: 0.9979622699647674[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   31.3s finished\n",
      "\n",
      "[2022-11-25 01:25:58] Features: 9/10 -- score: 0.9977074928947036[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:   29.2s finished\n",
      "\n",
      "[2022-11-25 01:26:28] Features: 10/10 -- score: 0.9978349625172818"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "forward_fs = sfs(rf , k_features=10,forward=True,floating=False,verbose=2,scoring='accuracy',cv=5)\n",
    "forward_fs = forward_fs.fit(X_train_resample, Y_train_resample_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TSH', 'TT4', 'FTI', 'on_thyroxine', 'on_antithyroid_medication', 'sick', 'goitre', 'FTI_measured', 'TBG_measured', 'referral_source']\n"
     ]
    }
   ],
   "source": [
    "feat_names = list(forward_fs.k_feature_names_)\n",
    "print(feat_names)\n",
    "X_train_new=X_train_resample[['age','sex','TSH', 'TT4', 'FTI', 'on_thyroxine', 'on_antithyroid_medication','sick', 'goitre', 'hypopituitary', 'psych', 'TBG_measured']]\n",
    "X_test_new=X_test_resample[['age','sex','TSH', 'TT4', 'FTI', 'on_thyroxine', 'on_antithyroid_medication','sick', 'goitre', 'hypopituitary', 'psych', 'TBG_measured']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "===============\n",
      "99%\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "                     0            1            2       3  accuracy  \\\n",
      "precision     0.994428     1.000000     0.998474     1.0  0.998217   \n",
      "recall        1.000000     0.992868     1.000000     1.0  0.998217   \n",
      "f1-score      0.997206     0.996421     0.999236     1.0  0.998217   \n",
      "support    1963.000000  1963.000000  1963.000000  1963.0  0.998217   \n",
      "\n",
      "             macro avg  weighted avg  \n",
      "precision     0.998225      0.998225  \n",
      "recall        0.998217      0.998217  \n",
      "f1-score      0.998216      0.998216  \n",
      "support    7852.000000   7852.000000  \n",
      "-----------------------------------\n",
      "Test Result:\n",
      "===============\n",
      "Accuracy Score:97.83%\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "                    0           1           2    3  accuracy    macro avg  \\\n",
      "precision    0.942529    1.000000    1.000000  0.0   0.97832     0.735632   \n",
      "recall       1.000000    0.991870    0.943089  0.0   0.97832     0.733740   \n",
      "f1-score     0.970414    0.995918    0.970711  0.0   0.97832     0.734261   \n",
      "support    492.000000  492.000000  492.000000  0.0   0.97832  1476.000000   \n",
      "\n",
      "           weighted avg  \n",
      "precision      0.980843  \n",
      "recall         0.978320  \n",
      "f1-score       0.979015  \n",
      "support     1476.000000  \n",
      "---------------------------------\n",
      "Confusion Matrix:\n",
      "[[492   0   0   0]\n",
      " [  2 488   0   2]\n",
      " [ 28   0 464   0]\n",
      " [  0   0   0   0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "rf_model=rf.fit(X_train_new,Y_train_resample_flat)\n",
    "#Checking the metrics of Random Forest\n",
    "def print_Score(clf,x_train,x_test,y_train,y_test,train=True):\n",
    "    if train:\n",
    "        pred=clf.predict(x_train)\n",
    "        clf_report=pd.DataFrame(classification_report(y_train,pred,output_dict=True))\n",
    "        print(\"Train Result:\\n===============\")\n",
    "        print(str(int(accuracy_score(y_train,pred)*100))+\"%\")\n",
    "        print(\"---------------------------------\")\n",
    "        print(f\"Classification Report:\\n{clf_report}\")\n",
    "        print(\"-----------------------------------\")\n",
    "        \n",
    "       \n",
    "    elif train==False:\n",
    "        pred=clf.predict(x_test)\n",
    "        clf_report=pd.DataFrame(classification_report(y_test,pred,output_dict=True))\n",
    "        print(\"Test Result:\\n===============\")\n",
    "        print(f\"Accuracy Score:{accuracy_score(y_test,pred)*100:.2f}%\")\n",
    "        print(\"---------------------------------\")\n",
    "        print(f\"Classification Report:\\n{clf_report}\")\n",
    "        print(\"---------------------------------\")\n",
    "        print(f\"Confusion Matrix:\\n{confusion_matrix(y_test,pred)}\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "print_Score(rf_model,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=True)\n",
    "print_Score(rf_model,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 14:42:24,480 INFO:Train Result:\n",
      "===============\n",
      "Accuracy Score:100.00%\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "                0       1       2       3  accuracy  macro avg  weighted avg\n",
      "precision     1.0     1.0     1.0     1.0       1.0        1.0           1.0\n",
      "recall        1.0     1.0     1.0     1.0       1.0        1.0           1.0\n",
      "f1-score      1.0     1.0     1.0     1.0       1.0        1.0           1.0\n",
      "support    1963.0  1963.0  1963.0  1963.0       1.0     7852.0        7852.0\n",
      "-----------------------------------\n",
      "Confusion Matrix:\n",
      "[[1963    0    0    0]\n",
      " [   0 1963    0    0]\n",
      " [   0    0 1963    0]\n",
      " [   0    0    0 1963]]\n",
      "\n",
      "2021-12-29 14:42:24,580 INFO:Test Result:\n",
      "===============\n",
      "Accuracy Score:98.10%\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "                    0           1           2  accuracy    macro avg  \\\n",
      "precision    0.946154    1.000000    1.000000   0.98103     0.982051   \n",
      "recall       1.000000    0.995935    0.947154   0.98103     0.981030   \n",
      "f1-score     0.972332    0.997963    0.972860   0.98103     0.981052   \n",
      "support    492.000000  492.000000  492.000000   0.98103  1476.000000   \n",
      "\n",
      "           weighted avg  \n",
      "precision      0.982051  \n",
      "recall         0.981030  \n",
      "f1-score       0.981052  \n",
      "support     1476.000000  \n",
      "---------------------------------\n",
      "Confusion Matrix:\n",
      "[[492   0   0]\n",
      " [  2 490   0]\n",
      " [ 26   0 466]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "===============\n",
      "Accuracy Score:100.00%\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "                0       1       2       3  accuracy  macro avg  weighted avg\n",
      "precision     1.0     1.0     1.0     1.0       1.0        1.0           1.0\n",
      "recall        1.0     1.0     1.0     1.0       1.0        1.0           1.0\n",
      "f1-score      1.0     1.0     1.0     1.0       1.0        1.0           1.0\n",
      "support    1963.0  1963.0  1963.0  1963.0       1.0     7852.0        7852.0\n",
      "-----------------------------------\n",
      "Confusion Matrix:\n",
      "[[1963    0    0    0]\n",
      " [   0 1963    0    0]\n",
      " [   0    0 1963    0]\n",
      " [   0    0    0 1963]]\n",
      "\n",
      "Test Result:\n",
      "===============\n",
      "Accuracy Score:98.10%\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "                    0           1           2  accuracy    macro avg  \\\n",
      "precision    0.946154    1.000000    1.000000   0.98103     0.982051   \n",
      "recall       1.000000    0.995935    0.947154   0.98103     0.981030   \n",
      "f1-score     0.972332    0.997963    0.972860   0.98103     0.981052   \n",
      "support    492.000000  492.000000  492.000000   0.98103  1476.000000   \n",
      "\n",
      "           weighted avg  \n",
      "precision      0.982051  \n",
      "recall         0.981030  \n",
      "f1-score       0.981052  \n",
      "support     1476.000000  \n",
      "---------------------------------\n",
      "Confusion Matrix:\n",
      "[[492   0   0]\n",
      " [  2 490   0]\n",
      " [ 26   0 466]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Hyper parameter tuning\n",
    "RF=RandomForestClassifier()\n",
    "model=RF.fit(X_train_new,Y_train_resample_flat)\n",
    "\n",
    "print_Score(model,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=True)\n",
    "print_Score(model,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [10, 120, 230, 340, 450, 560, 670, 780, 890, 1000], 'min_samples_split': [1, 3, 4, 5, 7, 9], 'min_samples_leaf': [1, 2, 4, 6, 8], 'criterion': ['entropy', 'gini']}\n"
     ]
    }
   ],
   "source": [
    "n_estimators=[int(x) for x in np.linspace(start=200,stop=2000,num=10)]\n",
    "#No of features consider at every split\n",
    "max_features=['auto','sqrt','log2']\n",
    "#maximum no of levels in trees\n",
    "max_depth=[int(x) for x in np.linspace(10,1000,10)]\n",
    "#minimum no of samples required to split a node\n",
    "min_samples_split=[1,3,4,5,7,9]\n",
    "#minimum samples leafs required at each leaf node\n",
    "min_sample_leafs=[1,2,4,6,8]\n",
    "\n",
    "\n",
    "random_grid={'n_estimators':n_estimators,\n",
    "'max_features':max_features,\n",
    "'max_depth':max_depth,\n",
    "'min_samples_split':min_samples_split,\n",
    "'min_samples_leaf':min_sample_leafs,\n",
    "'criterion':['entropy','gini']}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 467, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 254, in fit\n",
      "    % self.min_samples_split\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\SOUMALLA-ANTARA\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.99796238 0.99477847        nan 0.99566983 0.9960519  0.99796233\n",
      " 0.9960519  0.99643396 0.9955425  0.99681603 0.99503316 0.99477842\n",
      "        nan 0.99516044        nan 0.99566983 0.99796233 0.99783501\n",
      "        nan 0.99477842 0.99783501 0.99796238 0.99808971 0.99796238\n",
      " 0.99796238        nan        nan 0.99668866 0.99528781        nan\n",
      " 0.99554246        nan        nan 0.99796233 0.99490574 0.99808971\n",
      " 0.99783501 0.99617922 0.99758022 0.99796233 0.99452372 0.99796238\n",
      " 0.99490584 0.99745289 0.99592452 0.9971981  0.99796238 0.99541518\n",
      "        nan 0.99796238 0.99796238 0.99592443 0.99758027 0.99541513\n",
      " 0.99796238 0.99592452 0.99681608 0.99783501 0.99783501 0.99566983\n",
      " 0.99796233 0.99808971 0.99796238        nan 0.99745289 0.99452372\n",
      " 0.99452372        nan 0.99808971 0.99554246 0.99796238 0.99796238\n",
      " 0.99745289 0.99694345 0.99808971 0.99656134 0.99808971 0.99668866\n",
      " 0.99796233 0.99796238 0.99808971 0.99796233 0.99796233 0.99796233\n",
      "        nan 0.99808971 0.99707073 0.99503311        nan 0.99477847\n",
      " 0.99745289 0.99796238 0.99656134 0.99796233 0.99452372 0.99796238\n",
      " 0.99490579 0.99783501        nan 0.99592448]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['entropy', 'gini'],\n",
       "                                        'max_depth': [10, 120, 230, 340, 450,\n",
       "                                                      560, 670, 780, 890,\n",
       "                                                      1000],\n",
       "                                        'max_features': ['auto', 'sqrt',\n",
       "                                                         'log2'],\n",
       "                                        'min_samples_leaf': [1, 2, 4, 6, 8],\n",
       "                                        'min_samples_split': [1, 3, 4, 5, 7, 9],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   random_state=0, verbose=2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv=RandomizedSearchCV(estimator=RandomForestClassifier(),param_distributions=random_grid,n_iter=100,cv=3,verbose=2,random_state=0,n_jobs=-1)\n",
    "\n",
    "rcv.fit(X_train_new,Y_train_resample_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=230, min_samples_leaf=2,\n",
       "                       min_samples_split=3, n_estimators=400)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "===============\n",
      "Accuracy Score:97.97%\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "                    0           1           2  accuracy    macro avg  \\\n",
      "precision    0.942529    1.000000    1.000000  0.979675     0.980843   \n",
      "recall       1.000000    0.995935    0.943089  0.979675     0.979675   \n",
      "f1-score     0.970414    0.997963    0.970711  0.979675     0.979696   \n",
      "support    492.000000  492.000000  492.000000  0.979675  1476.000000   \n",
      "\n",
      "           weighted avg  \n",
      "precision      0.980843  \n",
      "recall         0.979675  \n",
      "f1-score       0.979696  \n",
      "support     1476.000000  \n",
      "---------------------------------\n",
      "Confusion Matrix:\n",
      "[[492   0   0]\n",
      " [  2 490   0]\n",
      " [ 28   0 464]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_random_grid=rcv.best_estimator_\n",
    "\n",
    "print_Score(best_random_grid,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion': [rcv.best_params_['criterion']],\n",
    "    'max_depth': [rcv.best_params_['max_depth']],\n",
    "    'max_features': [rcv.best_params_['max_features']],\n",
    "    'min_samples_leaf': [rcv.best_params_['min_samples_leaf'], \n",
    "                         rcv.best_params_['min_samples_leaf']+2],\n",
    "    'min_samples_split': [rcv.best_params_['min_samples_split'] - 1,\n",
    "                          rcv.best_params_['min_samples_split'], \n",
    "                          rcv.best_params_['min_samples_split'] +1],\n",
    "    'n_estimators': [rcv.best_params_['n_estimators'] - 100, \n",
    "                     rcv.best_params_['n_estimators'], \n",
    "                     rcv.best_params_['n_estimators'] + 200]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': ['entropy'], 'max_depth': [230], 'max_features': ['auto'], 'min_samples_leaf': [2, 4], 'min_samples_split': [2, 3, 4], 'n_estimators': [300, 400, 600]}\n"
     ]
    }
   ],
   "source": [
    "print(param_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=600; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=600; total time=   3.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=600; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=600; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=600; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=600; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=600; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=600; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=600; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=600; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=300; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=400; total time=   2.1s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=400; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=400; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=600; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=600; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=600; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=600; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=600; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=600; total time=   3.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=600; total time=   3.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=600; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=600; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=3, n_estimators=600; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=300; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=300; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=300; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=300; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=300; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=300; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=300; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=300; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=400; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=400; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=400; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=400; total time=   2.2s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=400; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=600; total time=   3.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=600; total time=   3.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=600; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=600; total time=   3.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=600; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=600; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=600; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=600; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=600; total time=   3.5s\n",
      "[CV] END criterion=entropy, max_depth=230, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=600; total time=   3.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=RandomForestClassifier(), n_jobs=1,\n",
       "             param_grid={'criterion': ['entropy'], 'max_depth': [230],\n",
       "                         'max_features': ['auto'], 'min_samples_leaf': [2, 4],\n",
       "                         'min_samples_split': [2, 3, 4],\n",
       "                         'n_estimators': [300, 400, 600]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search=GridSearchCV(estimator=RandomForestClassifier(),param_grid=param_grid,cv=10,n_jobs=1,verbose=2)\n",
    "grid_search.fit(X_train_new,Y_train_resample_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=230, min_samples_leaf=2,\n",
       "                       n_estimators=600)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_grid=grid_search.best_estimator_\n",
    "best_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "===============\n",
      "Accuracy Score:97.97%\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "                    0           1           2  accuracy    macro avg  \\\n",
      "precision    0.942529    1.000000    1.000000  0.979675     0.980843   \n",
      "recall       1.000000    0.995935    0.943089  0.979675     0.979675   \n",
      "f1-score     0.970414    0.997963    0.970711  0.979675     0.979696   \n",
      "support    492.000000  492.000000  492.000000  0.979675  1476.000000   \n",
      "\n",
      "           weighted avg  \n",
      "precision      0.980843  \n",
      "recall         0.979675  \n",
      "f1-score       0.979696  \n",
      "support     1476.000000  \n",
      "---------------------------------\n",
      "Confusion Matrix:\n",
      "[[492   0   0]\n",
      " [  2 490   0]\n",
      " [ 28   0 464]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_Score(best_grid,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_out=open('Thyroid_model.pkl','wb')\n",
    "pickle.dump(grid_search,pickle_out)\n",
    "pickle_out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9ce4bfd38a205b688ce9e2dff57748d8f467780e3e34d2409e973c80fb77dee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
